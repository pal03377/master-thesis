@book{bruegge2004object,
	title = {Object Oriented Software Engineering Using UML, Patterns, and Java},
	author = {Bruegge, Bernd and Dutoit, Allen H},
	year = {2009},
	publisher = {Prentice Hall}
}


@inproceedings{cofee,
  author    = {Bernius, Jan Philip and Krusche, Stephan and Bruegge, Bernd},
  title     = {A Machine Learning Approach for Suggesting Feedback in Textual Exercises in Large Courses},
  year      = {2021},
  isbn      = {9781450382151},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3430895.3460135},
  doi       = {10.1145/3430895.3460135},
  abstract  = {Open-ended textual exercises facilitate the comprehension of problem-solving skills. Students can learn from their mistakes when teachers provide individual feedback. However, courses with hundreds of students cause a heavy workload for teachers: providing individual feedback is mostly a manual, repetitive, and time-consuming activity.This paper presents CoFee, a machine learning approach designed to suggest computer-aided feedback in open-ended textual exercises. The approach uses topic modeling to split student answers into text segments and language embeddings to transform these segments. It then applies clustering to group the text segments by similarity so that the same feedback can be applied to all segments within the same cluster.We implemented this approach in a reference implementation called Athene and integrated it into Artemis. We used Athene to review 17 textual exercises in two large courses at the Technical University of Munich with 2,300 registered students and 53 teachers. On average, Athene suggested feedback for 26% of the submissions. Accordingly, 85% of these suggestions were accepted by the teachers, 5% were extended with a comment and then accepted, and 10% were changed.},
  booktitle = {Proceedings of the Eighth ACM Conference on Learning @ Scale},
  pages     = {173–182},
  numpages  = {10},
  keywords  = {software engineering, grading, education, learning, interactive learning, feedback, assessment support system, automatic assessment},
  location  = {Virtual Event, Germany},
  series    = {L@S '21}
}

@article{cofee2,
  abstract   = {Many engineering disciplines require problem-solving skills, which cannot be learned by memorization alone. Open-ended textual exercises allow students to acquire these skills. Students can learn from their mistakes when instructors provide individual feedback. However, grading these exercises is often a manual, repetitive, and time-consuming activity. The number of computer science students graduating per year has steadily increased over the last decade. This rise has led to large courses that cause a heavy workload for instructors, especially if they provide individual feedback to students. This article presents CoFee, a framework to generate and suggest computer-aided feedback for textual exercises based on machine learning. CoFee utilizes a segment-based grading concept, which links feedback to text segments. CoFee automates grading based on topic modeling and an assessment knowledge repository acquired during previous assessments. A language model builds an intermediate representation of the text segments. Hierarchical clustering identifies groups of similar text segments to reduce the grading overhead. We first demonstrated the CoFee framework in a small laboratory experiment in 2019, which showed that the grading overhead could be reduced by 85%. This experiment confirmed the feasibility of automating the grading process for problem-solving exercises. We then evaluated CoFee in a large course at the Technical University of Munich from 2019 to 2021, with up to 2, 200 enrolled students per course. We collected data from 34 exercises offered in each of these courses. On average, CoFee suggested feedback for 45% of the submissions. 92% (Positive Predictive Value) of these suggestions were precise and, therefore, accepted by the instructors.},
  author     = {Bernius, Jan Philip and Krusche, Stephan and Bruegge, Bernd},
  doi        = {https://doi.org/10.1016/j.caeai.2022.100081},
  issn       = {2666-920X},
  journal    = {Computers and Education: Artificial Intelligence},
  keywords   = {Software engineering, Education, Interactive learning, Automatic assessment, Grading, Assessment support system, Learning, Feedback},
  pages      = {100081},
  title      = {Machine learning based feedback on textual student answers in large courses},
  url        = {https://www.sciencedirect.com/science/article/pii/S2666920X22000364},
  volume     = {3},
  year       = {2022},
  bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S2666920X22000364},
  bdsk-url-2 = {https://doi.org/10.1016/j.caeai.2022.100081}
}

@inproceedings{singh2013automated,
  title        = {Automated feedback generation for introductory programming assignments},
  author       = {Singh, Rishabh and Gulwani, Sumit and Solar-Lezama, Armando},
  booktitle    = {ACM SIGPLAN Notices},
  volume       = {48},
  number       = {6},
  pages        = {15–26},
  year         = {2013},
  organization = {ACM},
  url          = {https://people.csail.mit.edu/rishabh/papers/autograderPLDI13.pdf}
}

@inproceedings{compass,
  author = {Krusche, Stephan},
  year   = {2022},
  month  = {01},
  pages  = {},
  title  = {Semi-Automatic Assessment of Modeling Exercises using Supervised Machine Learning},
  doi    = {10.24251/HICSS.2022.108}
}

@inproceedings{messer2022grading,
  abstract  = {Over the last few years, Computer Science class sizes have increased, resulting in a higher grading workload. Universities often use multiple graders to quickly deliver the grades and associated feedback to manage this workload. While using multiple graders enables the required turnaround times to be achieved, it can come at the cost of consistency and feedback quality. Partially automating the process of grading and feedback could help solve these issues. This project will look into methods to assist in grading and feedback partially subjective elements of programming assignments, such as readability, maintainability, and documentation, to increase the marker's amount of time to write meaningful feedback. We will investigate machine learning and natural language processing methods to improve grade uniformity and feedback quality in these areas. Furthermore, we will investigate how using these tools may allow instructors to include open-ended requirements that challenge students to use their ideas for possible features in their assignments.},
  address   = {Cham},
  author    = {Messer, Marcus},
  booktitle = {Artificial Intelligence in Education. Posters and Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks, Practitioners' and Doctoral Consortium},
  editor    = {Rodrigo, Maria Mercedes and Matsuda, Noburu and Cristea, Alexandra I. and Dimitrova, Vania},
  isbn      = {978-3-031-11647-6},
  pages     = {35--40},
  publisher = {Springer International Publishing},
  title     = {Grading Programming Assignments with an Automated Grading and Feedback Assistant},
  year      = {2022}
}

@inproceedings{chow2017automated,
  author    = {Chow, Sammi and Yacef, Kalina and Koprinska, Irena and Curran, James},
  title     = {Automated Data-Driven Hints for Computer Programming Students},
  year      = {2017},
  isbn      = {9781450350679},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3099023.3099065},
  doi       = {10.1145/3099023.3099065},
  abstract  = {Formative feedback is essential for learning computer programming but is also a challenge to automate because of the many solutions a programming exercise can have. Whilst programming tutoring systems can easily generate automated feedback on how correct a program is, they less often provide some personalised guidance on how to improve or fix the code. In this paper, we present an approach for generating hints using previous student data. Utilising a range of techniques such as filtering, clustering and pattern mining, four different types of data-driven hints are generated: input suggestion, code-based, concept and pre-emptive hints. We evaluated our approach with data from 5529 students using the Grok Learning platform for teaching programming in Python. The results show that we can generate various types of hints for over 90\% of students with data from only 10 students, and hence, reduce the cold-start problem.},
  booktitle = {Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization},
  pages     = {5–10},
  numpages  = {6},
  keywords  = {educational data mining, intelligent teaching systems, personalised feedback},
  location  = {Bratislava, Slovakia},
  series    = {UMAP '17}
}

@article{sourceCodeAssessment,
  abstract       = {The rate of software development has increased dramatically. Conventional compilers cannot assess and detect all source code errors. Software may thus contain errors, negatively affecting end-users. It is also difficult to assess and detect source code logic errors using traditional compilers, resulting in software that contains errors. A method that utilizes artificial intelligence for assessing and detecting errors and classifying source code as correct (error-free) or incorrect is thus required. Here, we propose a sequential language model that uses an attention-mechanism-based long short-term memory (LSTM) neural network to assess and classify source code based on the estimated error probability. The attentive mechanism enhances the accuracy of the proposed language model for error assessment and classification. We trained the proposed model using correct source code and then evaluated its performance. The experimental results show that the proposed model has logic and syntax error detection accuracies of 92.2% and 94.8%, respectively, outperforming state-of-the-art models. We also applied the proposed model to the classification of source code with logic and syntax errors. The average precision, recall, and F-measure values for such classification are much better than those of benchmark models. To strengthen the proposed model, we combined the attention mechanism with LSTM to enhance the results of error assessment and detection as well as source code classification. Finally, our proposed model can be effective in programming education and software engineering by improving code writing, debugging, error-correction, and reasoning.},
  article-number = {2973},
  author         = {Rahman, Md. Mostafizer and Watanobe, Yutaka and Nakamura, Keita},
  doi            = {10.3390/app10082973},
  issn           = {2076-3417},
  journal        = {Applied Sciences},
  number         = {8},
  title          = {Source Code Assessment and Classification Based on Estimated Error Probability Using Attentive LSTM Language Model and Its Application in Programming Education},
  url            = {https://www.mdpi.com/2076-3417/10/8/2973},
  volume         = {10},
  year           = {2020},
  bdsk-url-1     = {https://www.mdpi.com/2076-3417/10/8/2973},
  bdsk-url-2     = {https://doi.org/10.3390/app10082973}
}

@misc{atheneTracking,
  author = {Petry, Jonas},
  title  = {Exercise Assessment Management in Artemis by Students and Instructors},
  year   = {2020},
  note   = {Bachelor`s thesis at Technical University of Munich}
}

@misc{atheneLoadBalancer,
  author = {Michel, Linus},
  title  = {Optimizing and Scaling Automatic Assessments of Textual Exercises for Very Large Lectures},
  year   = {2020},
  note   = {Master`s thesis at Technical University of Munich}
}

@misc{atheneLanguage,
  author = {Cremer, Tim},
  title  = {Language Independent Text Assessment},
  year   = {2022},
  note   = {Bachelor`s thesis at Technical University of Munich}
}

@inproceedings{elmo,
  title     = {Deep Contextualized Word Representations},
  author    = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  month     = jun,
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N18-1202},
  doi       = {10.18653/v1/N18-1202},
  pages     = {2227--2237},
  abstract  = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.}
}

@misc{codeBERT,
  doi       = {10.48550/ARXIV.2002.08155},
  url       = {https://arxiv.org/abs/2002.08155},
  author    = {Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and Zhou, Ming},
  keywords  = {Computation and Language (cs.CL), Programming Languages (cs.PL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{ArTEMiS,
  author    = {Krusche, Stephan and Seitz, Andreas},
  doi       = {10.1145/3159450.3159602},
  pages     = {284--289},
  publisher = {ACM},
  series    = {49th Technical Symposium on Computer Science Education},
  title     = {{ArTEMiS: An Automatic Assessment Management System for Interactive Learning}},
  year      = {2018}
}

@inproceedings{hdbscan,
  doi       = {10.1109/icdmw.2017.12},
  url       = {https://doi.org/10.1109%2Ficdmw.2017.12},
  year      = 2017,
  month     = {nov},
  publisher = {{IEEE}
               },
  author    = {Leland McInnes and John Healy},
  title     = {Accelerated Hierarchical Density Based Clustering},
  booktitle = {2017 {IEEE} International Conference on Data Mining Workshops ({ICDMW})}
}

@article{atenea2005,
  author   = { Pérez, Diana amd Alfonseca, Enrique and Rodríguez, Pilar and Gliozzo, Alfio and Strapparava, Carlo and Magnini, Bernardo},
  title    = { About the effects of combining Latent Semantic Analysis with natural language processing techniques for free-text assessment },
  year     = { 2005 },
  journal  = { Revista Signos },
  keywords = { LSA; free-text assessment; computer assisted assessment; e-learning; LSA; preguntas abiertas; evaluación asistida por ordenador; e-learning },
  issn     = { 0035-0451 },
  language = {Español},
  url      = {https://www.redalyc.org/articulo.oa?id=157013769004 },
  abstract = { }
}

@inproceedings{alikaniotis2016,
  title     = {Automatic Text Scoring Using Neural Networks},
  author    = {Alikaniotis, Dimitrios  and
               Yannakoudakis, Helen  and
               Rei, Marek},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = aug,
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P16-1068},
  doi       = {10.18653/v1/P16-1068},
  pages     = {715--725}
}

@article{morris2021formative,
  author   = {Morris, Rebecca and Perry, Thomas and Wardle, Lindsey},
  title    = {Formative assessment and feedback for learning in higher education: A systematic review},
  journal  = {Review of Education},
  volume   = {9},
  number   = {3},
  pages    = {e3292},
  keywords = {evidence-informed practice, feedback, formative assessment, higher education, systematic review},
  doi      = {https://doi.org/10.1002/rev3.3292},
  url      = {https://bera-journals.onlinelibrary.wiley.com/doi/abs/10.1002/rev3.3292},
  eprint   = {https://bera-journals.onlinelibrary.wiley.com/doi/pdf/10.1002/rev3.3292},
  abstract = {Abstract Feedback is an integral part of education and there is a substantial body of trials exploring and confirming its effect on learning. This evidence base comes mostly from studies of compulsory school age children; there is very little evidence to support effective feedback practice at higher education, beyond the frameworks and strategies advocated by those claiming expertise in the area. This systematic review aims to address this gap. We review causal evidence from trials of feedback and formative assessment in higher education. Although the evidence base is currently limited, our results suggest that low stakes-quizzing is a particularly powerful approach and that there are benefits for forms of peer and tutor feedback, although these depend on implementation factors. There was mixed evidence for praise, grading and technology-based feedback. We organise our findings into several evidence-grounded categories and discuss the next steps for the field and evidence-informed feedback practice in universities. Context and implications Rationale for this study To gain a better understanding of effective formative assessment and feedback approaches in higher education (HE). To promote a more evidence-informed approach to teaching and learning in universities. Why the new findings matter The findings highlight a small number of promising strategies for formative assessment and feedback in HE. They also draw attention to a lack of (quality) evidence in this area overall. Implications for policy-makers and practitioners Universities and their regulators/funders should be encouraging and supporting more, high-quality research in this important area. Researchers in the field also need to look to developing more ambitious, higher-quality studies which are likely to provide robust, causal conclusions about academic effectiveness (or other outcomes). Those involved in teaching and learning in university should use the findings to inform evidence-informed approaches to formative assessment and feedback and to challenge approaches which do not appear to have foundations in strong evidence. Students could be made more aware of teaching and learning approaches that are likely to support their academic progress.},
  year     = {2021}
}

@article{hattie2007feedback,
  author   = {Hattie, John and Timperley, Helen},
  title    = {The Power of Feedback},
  journal  = {Review of Educational Research},
  volume   = {77},
  number   = {1},
  pages    = {81-112},
  year     = {2007},
  doi      = {10.3102/003465430298487},
  url      = {https://doi.org/10.3102/003465430298487},
  eprint   = {https://doi.org/10.3102/003465430298487},
  abstract = { Feedback is one of the most powerful influences on learning and achievement, but this impact can be either positive or negative. Its power is frequently mentioned in articles about learning and teaching, but surprisingly few recent studies have systematically investigated its meaning. This article provides a conceptual analysis of feedback and reviews the evidence related to its impact on learning and achievement. This evidence shows that although feedback is among the major influences, the type of feedback and the way it is given can be differentially effective. A model of feedback is then proposed that identifies the particular properties and circumstances that make it effective, and some typically thorny issues are discussed, including the timing of feedback and the effects of positive and negative feedback. Finally, this analysis is used to suggest ways in which feedback can be used to enhance its effectiveness in classrooms. }
}

@article{keuning2018review,
  author     = {Keuning, Hieke and Jeuring, Johan and Heeren, Bastiaan},
  title      = {A Systematic Literature Review of Automated Feedback Generation for Programming Exercises},
  year       = {2018},
  issue_date = {March 2019},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {19},
  number     = {1},
  url        = {https://doi.org/10.1145/3231711},
  doi        = {10.1145/3231711},
  abstract   = {Formative feedback, aimed at helping students to improve their work, is an important factor in learning. Many tools that offer programming exercises provide automated feedback on student solutions. We have performed a systematic literature review to find out what kind of feedback is provided, which techniques are used to generate the feedback, how adaptable the feedback is, and how these tools are evaluated. We have designed a labelling to classify the tools, and use Narciss’ feedback content categories to classify feedback messages. We report on the results of coding a total of 101 tools. We have found that feedback mostly focuses on identifying mistakes and less on fixing problems and taking a next step. Furthermore, teachers cannot easily adapt tools to their own needs. However, the diversity of feedback types has increased over the past decades and new techniques are being applied to generate feedback that is increasingly helpful for students.},
  journal    = {ACM Trans. Comput. Educ.},
  month      = {sep},
  articleno  = {3},
  numpages   = {43},
  keywords   = {learning programming, programming tools, Systematic literature review, automated feedback}
}