@book{bruegge2004object,
	title = {Object Oriented Software Engineering Using UML, Patterns, and Java},
	author = {Bruegge, Bernd and Dutoit, Allen H},
	year = {2009},
	publisher = {Prentice Hall}
}


@inproceedings{cofee,
  author    = {Bernius, Jan Philip and Krusche, Stephan and Bruegge, Bernd},
  title     = {A Machine Learning Approach for Suggesting Feedback in Textual Exercises in Large Courses},
  year      = {2021},
  isbn      = {9781450382151},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3430895.3460135},
  doi       = {10.1145/3430895.3460135},
  abstract  = {Open-ended textual exercises facilitate the comprehension of problem-solving skills. Students can learn from their mistakes when teachers provide individual feedback. However, courses with hundreds of students cause a heavy workload for teachers: providing individual feedback is mostly a manual, repetitive, and time-consuming activity.This paper presents CoFee, a machine learning approach designed to suggest computer-aided feedback in open-ended textual exercises. The approach uses topic modeling to split student answers into text segments and language embeddings to transform these segments. It then applies clustering to group the text segments by similarity so that the same feedback can be applied to all segments within the same cluster.We implemented this approach in a reference implementation called Athene and integrated it into Artemis. We used Athene to review 17 textual exercises in two large courses at the Technical University of Munich with 2,300 registered students and 53 teachers. On average, Athene suggested feedback for 26% of the submissions. Accordingly, 85% of these suggestions were accepted by the teachers, 5% were extended with a comment and then accepted, and 10% were changed.},
  booktitle = {Proceedings of the Eighth ACM Conference on Learning @ Scale},
  pages     = {173–182},
  numpages  = {10},
  keywords  = {software engineering, grading, education, learning, interactive learning, feedback, assessment support system, automatic assessment},
  location  = {Virtual Event, Germany},
  series    = {L@S '21}
}

@article{cofee2,
  abstract   = {Many engineering disciplines require problem-solving skills, which cannot be learned by memorization alone. Open-ended textual exercises allow students to acquire these skills. Students can learn from their mistakes when instructors provide individual feedback. However, grading these exercises is often a manual, repetitive, and time-consuming activity. The number of computer science students graduating per year has steadily increased over the last decade. This rise has led to large courses that cause a heavy workload for instructors, especially if they provide individual feedback to students. This article presents CoFee, a framework to generate and suggest computer-aided feedback for textual exercises based on machine learning. CoFee utilizes a segment-based grading concept, which links feedback to text segments. CoFee automates grading based on topic modeling and an assessment knowledge repository acquired during previous assessments. A language model builds an intermediate representation of the text segments. Hierarchical clustering identifies groups of similar text segments to reduce the grading overhead. We first demonstrated the CoFee framework in a small laboratory experiment in 2019, which showed that the grading overhead could be reduced by 85%. This experiment confirmed the feasibility of automating the grading process for problem-solving exercises. We then evaluated CoFee in a large course at the Technical University of Munich from 2019 to 2021, with up to 2, 200 enrolled students per course. We collected data from 34 exercises offered in each of these courses. On average, CoFee suggested feedback for 45% of the submissions. 92% (Positive Predictive Value) of these suggestions were precise and, therefore, accepted by the instructors.},
  author     = {Bernius, Jan Philip and Krusche, Stephan and Bruegge, Bernd},
  doi        = {https://doi.org/10.1016/j.caeai.2022.100081},
  issn       = {2666-920X},
  journal    = {Computers and Education: Artificial Intelligence},
  keywords   = {Software engineering, Education, Interactive learning, Automatic assessment, Grading, Assessment support system, Learning, Feedback},
  pages      = {100081},
  title      = {Machine learning based feedback on textual student answers in large courses},
  url        = {https://www.sciencedirect.com/science/article/pii/S2666920X22000364},
  volume     = {3},
  year       = {2022},
  bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S2666920X22000364},
  bdsk-url-2 = {https://doi.org/10.1016/j.caeai.2022.100081}
}

@inproceedings{singh2013automated,
  title        = {Automated feedback generation for introductory programming assignments},
  author       = {Singh, Rishabh and Gulwani, Sumit and Solar-Lezama, Armando},
  booktitle    = {ACM SIGPLAN Notices},
  volume       = {48},
  number       = {6},
  pages        = {15–26},
  year         = {2013},
  organization = {ACM},
  url          = {https://people.csail.mit.edu/rishabh/papers/autograderPLDI13.pdf}
}

@inproceedings{compass,
  author = {Krusche, Stephan},
  year   = {2022},
  month  = {01},
  pages  = {},
  title  = {Semi-Automatic Assessment of Modeling Exercises using Supervised Machine Learning},
  doi    = {10.24251/HICSS.2022.108}
}

@inproceedings{messer2022grading,
  abstract  = {Over the last few years, Computer Science class sizes have increased, resulting in a higher grading workload. Universities often use multiple graders to quickly deliver the grades and associated feedback to manage this workload. While using multiple graders enables the required turnaround times to be achieved, it can come at the cost of consistency and feedback quality. Partially automating the process of grading and feedback could help solve these issues. This project will look into methods to assist in grading and feedback partially subjective elements of programming assignments, such as readability, maintainability, and documentation, to increase the marker's amount of time to write meaningful feedback. We will investigate machine learning and natural language processing methods to improve grade uniformity and feedback quality in these areas. Furthermore, we will investigate how using these tools may allow instructors to include open-ended requirements that challenge students to use their ideas for possible features in their assignments.},
  address   = {Cham},
  author    = {Messer, Marcus},
  booktitle = {Artificial Intelligence in Education. Posters and Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks, Practitioners' and Doctoral Consortium},
  editor    = {Rodrigo, Maria Mercedes and Matsuda, Noburu and Cristea, Alexandra I. and Dimitrova, Vania},
  isbn      = {978-3-031-11647-6},
  pages     = {35--40},
  publisher = {Springer International Publishing},
  title     = {Grading Programming Assignments with an Automated Grading and Feedback Assistant},
  year      = {2022}
}

@inproceedings{chow2017automated,
  author    = {Chow, Sammi and Yacef, Kalina and Koprinska, Irena and Curran, James},
  title     = {Automated Data-Driven Hints for Computer Programming Students},
  year      = {2017},
  isbn      = {9781450350679},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3099023.3099065},
  doi       = {10.1145/3099023.3099065},
  abstract  = {Formative feedback is essential for learning computer programming but is also a challenge to automate because of the many solutions a programming exercise can have. Whilst programming tutoring systems can easily generate automated feedback on how correct a program is, they less often provide some personalised guidance on how to improve or fix the code. In this paper, we present an approach for generating hints using previous student data. Utilising a range of techniques such as filtering, clustering and pattern mining, four different types of data-driven hints are generated: input suggestion, code-based, concept and pre-emptive hints. We evaluated our approach with data from 5529 students using the Grok Learning platform for teaching programming in Python. The results show that we can generate various types of hints for over 90\% of students with data from only 10 students, and hence, reduce the cold-start problem.},
  booktitle = {Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization},
  pages     = {5–10},
  numpages  = {6},
  keywords  = {educational data mining, intelligent teaching systems, personalised feedback},
  location  = {Bratislava, Slovakia},
  series    = {UMAP '17}
}

@article{sourceCodeAssessment,
  abstract       = {The rate of software development has increased dramatically. Conventional compilers cannot assess and detect all source code errors. Software may thus contain errors, negatively affecting end-users. It is also difficult to assess and detect source code logic errors using traditional compilers, resulting in software that contains errors. A method that utilizes artificial intelligence for assessing and detecting errors and classifying source code as correct (error-free) or incorrect is thus required. Here, we propose a sequential language model that uses an attention-mechanism-based long short-term memory (LSTM) neural network to assess and classify source code based on the estimated error probability. The attentive mechanism enhances the accuracy of the proposed language model for error assessment and classification. We trained the proposed model using correct source code and then evaluated its performance. The experimental results show that the proposed model has logic and syntax error detection accuracies of 92.2% and 94.8%, respectively, outperforming state-of-the-art models. We also applied the proposed model to the classification of source code with logic and syntax errors. The average precision, recall, and F-measure values for such classification are much better than those of benchmark models. To strengthen the proposed model, we combined the attention mechanism with LSTM to enhance the results of error assessment and detection as well as source code classification. Finally, our proposed model can be effective in programming education and software engineering by improving code writing, debugging, error-correction, and reasoning.},
  article-number = {2973},
  author         = {Rahman, Md. Mostafizer and Watanobe, Yutaka and Nakamura, Keita},
  doi            = {10.3390/app10082973},
  issn           = {2076-3417},
  journal        = {Applied Sciences},
  number         = {8},
  title          = {Source Code Assessment and Classification Based on Estimated Error Probability Using Attentive LSTM Language Model and Its Application in Programming Education},
  url            = {https://www.mdpi.com/2076-3417/10/8/2973},
  volume         = {10},
  year           = {2020},
  bdsk-url-1     = {https://www.mdpi.com/2076-3417/10/8/2973},
  bdsk-url-2     = {https://doi.org/10.3390/app10082973}
}

@misc{atheneTracking,
  author = {Petry, Jonas},
  title  = {Exercise Assessment Management in Artemis by Students and Instructors},
  year   = {2020},
  howpublished = {Bachelor's thesis at Technical University of Munich}
}

@misc{atheneLoadBalancer,
  author = {Michel, Linus},
  title  = {Optimizing and Scaling Automatic Assessments of Textual Exercises for Very Large Lectures},
  year   = {2020},
  howpublished = {Master's thesis at Technical University of Munich}
}

@misc{atheneLanguage,
  author = {Cremer, Tim},
  title  = {Language Independent Text Assessment},
  year   = {2022},
  howpublished = {Bachelor's thesis at Technical University of Munich}
}

@misc{artemisStaticCodeAnalysis,
  author = {Klöss-Schuster, Stefan},
  title  = {Automatic Generation of Feedback using Static Code Analysis in Artemis},
  year   = {2020},
  howpublished = {Master's thesis at Technical University of Munich}
}

@misc{athenaLLMs,
  author = {Dietrich, Felix},
  title  = {Leveraging LLMs for Automated Feedback Generation on Exercises},
  year   = {2023},
  howpublished = {Master's thesis at Technical University of Munich}
}

@inproceedings{elmo,
  title     = {Deep Contextualized Word Representations},
  author    = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  month     = jun,
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N18-1202},
  doi       = {10.18653/v1/N18-1202},
  pages     = {2227--2237},
  abstract  = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.}
}

@misc{codeBERT,
  doi       = {10.48550/ARXIV.2002.08155},
  url       = {https://arxiv.org/abs/2002.08155},
  author    = {Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and Zhou, Ming},
  keywords  = {Computation and Language (cs.CL), Programming Languages (cs.PL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{ArTEMiS,
  author    = {Krusche, Stephan and Seitz, Andreas},
  doi       = {10.1145/3159450.3159602},
  pages     = {284--289},
  publisher = {ACM},
  series    = {49th Technical Symposium on Computer Science Education},
  title     = {{ArTEMiS: An Automatic Assessment Management System for Interactive Learning}},
  year      = {2018}
}

@inproceedings{hdbscan,
  doi       = {10.1109/icdmw.2017.12},
  url       = {https://doi.org/10.1109%2Ficdmw.2017.12},
  year      = 2017,
  month     = {nov},
  publisher = {{IEEE}
               },
  author    = {Leland McInnes and John Healy},
  title     = {Accelerated Hierarchical Density Based Clustering},
  booktitle = {2017 {IEEE} International Conference on Data Mining Workshops ({ICDMW})}
}

@article{atenea2005,
  author   = { Pérez, Diana amd Alfonseca, Enrique and Rodríguez, Pilar and Gliozzo, Alfio and Strapparava, Carlo and Magnini, Bernardo},
  title    = { About the effects of combining Latent Semantic Analysis with natural language processing techniques for free-text assessment },
  year     = { 2005 },
  journal  = { Revista Signos },
  keywords = { LSA; free-text assessment; computer assisted assessment; e-learning; LSA; preguntas abiertas; evaluación asistida por ordenador; e-learning },
  issn     = { 0035-0451 },
  language = {Español},
  url      = {https://www.redalyc.org/articulo.oa?id=157013769004 },
  abstract = { }
}

@inproceedings{alikaniotis2016,
  title     = {Automatic Text Scoring Using Neural Networks},
  author    = {Alikaniotis, Dimitrios and Yannakoudakis, Helen and Rei, Marek},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = aug,
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P16-1068},
  doi       = {10.18653/v1/P16-1068},
  pages     = {715--725}
}

@article{morris2021formative,
  author   = {Morris, Rebecca and Perry, Thomas and Wardle, Lindsey},
  title    = {Formative assessment and feedback for learning in higher education: A systematic review},
  journal  = {Review of Education},
  volume   = {9},
  number   = {3},
  pages    = {e3292},
  keywords = {evidence-informed practice, feedback, formative assessment, higher education, systematic review},
  doi      = {https://doi.org/10.1002/rev3.3292},
  url      = {https://bera-journals.onlinelibrary.wiley.com/doi/abs/10.1002/rev3.3292},
  eprint   = {https://bera-journals.onlinelibrary.wiley.com/doi/pdf/10.1002/rev3.3292},
  abstract = {Abstract Feedback is an integral part of education and there is a substantial body of trials exploring and confirming its effect on learning. This evidence base comes mostly from studies of compulsory school age children; there is very little evidence to support effective feedback practice at higher education, beyond the frameworks and strategies advocated by those claiming expertise in the area. This systematic review aims to address this gap. We review causal evidence from trials of feedback and formative assessment in higher education. Although the evidence base is currently limited, our results suggest that low stakes-quizzing is a particularly powerful approach and that there are benefits for forms of peer and tutor feedback, although these depend on implementation factors. There was mixed evidence for praise, grading and technology-based feedback. We organise our findings into several evidence-grounded categories and discuss the next steps for the field and evidence-informed feedback practice in universities. Context and implications Rationale for this study To gain a better understanding of effective formative assessment and feedback approaches in higher education (HE). To promote a more evidence-informed approach to teaching and learning in universities. Why the new findings matter The findings highlight a small number of promising strategies for formative assessment and feedback in HE. They also draw attention to a lack of (quality) evidence in this area overall. Implications for policy-makers and practitioners Universities and their regulators/funders should be encouraging and supporting more, high-quality research in this important area. Researchers in the field also need to look to developing more ambitious, higher-quality studies which are likely to provide robust, causal conclusions about academic effectiveness (or other outcomes). Those involved in teaching and learning in university should use the findings to inform evidence-informed approaches to formative assessment and feedback and to challenge approaches which do not appear to have foundations in strong evidence. Students could be made more aware of teaching and learning approaches that are likely to support their academic progress.},
  year     = {2021}
}

@article{hattie2007feedback,
  author   = {Hattie, John and Timperley, Helen},
  title    = {The Power of Feedback},
  journal  = {Review of Educational Research},
  volume   = {77},
  number   = {1},
  pages    = {81-112},
  year     = {2007},
  doi      = {10.3102/003465430298487},
  url      = {https://doi.org/10.3102/003465430298487},
  eprint   = {https://doi.org/10.3102/003465430298487},
  abstract = { Feedback is one of the most powerful influences on learning and achievement, but this impact can be either positive or negative. Its power is frequently mentioned in articles about learning and teaching, but surprisingly few recent studies have systematically investigated its meaning. This article provides a conceptual analysis of feedback and reviews the evidence related to its impact on learning and achievement. This evidence shows that although feedback is among the major influences, the type of feedback and the way it is given can be differentially effective. A model of feedback is then proposed that identifies the particular properties and circumstances that make it effective, and some typically thorny issues are discussed, including the timing of feedback and the effects of positive and negative feedback. Finally, this analysis is used to suggest ways in which feedback can be used to enhance its effectiveness in classrooms. }
}

@article{keuning2018review,
  author     = {Keuning, Hieke and Jeuring, Johan and Heeren, Bastiaan},
  title      = {A Systematic Literature Review of Automated Feedback Generation for Programming Exercises},
  year       = {2018},
  issue_date = {March 2019},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {19},
  number     = {1},
  url        = {https://doi.org/10.1145/3231711},
  doi        = {10.1145/3231711},
  abstract   = {Formative feedback, aimed at helping students to improve their work, is an important factor in learning. Many tools that offer programming exercises provide automated feedback on student solutions. We have performed a systematic literature review to find out what kind of feedback is provided, which techniques are used to generate the feedback, how adaptable the feedback is, and how these tools are evaluated. We have designed a labelling to classify the tools, and use Narciss’ feedback content categories to classify feedback messages. We report on the results of coding a total of 101 tools. We have found that feedback mostly focuses on identifying mistakes and less on fixing problems and taking a next step. Furthermore, teachers cannot easily adapt tools to their own needs. However, the diversity of feedback types has increased over the past decades and new techniques are being applied to generate feedback that is increasingly helpful for students.},
  journal    = {ACM Trans. Comput. Educ.},
  month      = {sep},
  articleno  = {3},
  numpages   = {43},
  keywords   = {learning programming, programming tools, Systematic literature review, automated feedback}
}

@misc{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models}, 
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year={2023},
  eprint={2302.13971},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@misc{openai2023gpt4,
  title         = {GPT-4 Technical Report},
  author        = {OpenAI},
  year          = {2023},
  eprint        = {2303.08774},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{crow2018intelligent,
  author = {Crow, Tyne and Luxton-Reilly, Andrew and Wünsche, Burkhard},
  year   = {2018},
  month  = {01},
  pages  = {53-62},
  title  = {Intelligent tutoring systems for programming education: a systematic review},
  doi    = {10.1145/3160489.3160492}
}

@article{hattie2007educational,
  author   = {Hattie, John and Timperley, Helen},
  title    = {The Power of Feedback},
  journal  = {Review of Educational Research},
  volume   = {77},
  number   = {1},
  pages    = {81-112},
  year     = {2007},
  doi      = {10.3102/003465430298487},
  url      = {https://doi.org/10.3102/003465430298487},
  eprint   = {https://doi.org/10.3102/003465430298487},
  abstract = { Feedback is one of the most powerful influences on learning and achievement, but this impact can be either positive or negative. Its power is frequently mentioned in articles about learning and teaching, but surprisingly few recent studies have systematically investigated its meaning. This article provides a conceptual analysis of feedback and reviews the evidence related to its impact on learning and achievement. This evidence shows that although feedback is among the major influences, the type of feedback and the way it is given can be differentially effective. A model of feedback is then proposed that identifies the particular properties and circumstances that make it effective, and some typically thorny issues are discussed, including the timing of feedback and the effects of positive and negative feedback. Finally, this analysis is used to suggest ways in which feedback can be used to enhance its effectiveness in classrooms. }
}

@article{sabilah2018openended,
  doi       = {10.1088/1742-6596/947/1/012032},
  url       = {https://dx.doi.org/10.1088/1742-6596/947/1/012032},
  year      = {2018},
  month     = {jan},
  publisher = {IOP Publishing},
  volume    = {947},
  number    = {1},
  pages     = {012032},
  author    = {I Sabilah and J T Manoy},
  title     = {The Use of Open-Ended Questions with Giving Feedback (OEQGF) for Effective Mathematic Learning},
  journal   = {Journal of Physics: Conference Series},
  abstract  = {Feedback deals with giving information to students related to their task which is done through score of their achievement, reaction, and comments. Considering its hierarchy, task difficulty level consists of low, middle and high levels. The difficulty level of open-ended questions is middle to high. Open-ended question is a good way to train students’ knowledge. This research is a descriptive research which aims at describing teacher’s learning management, students’ activities, students’ learning achievement, and students’ responses in mathematic learning using OEQGF. The subject was a teacher of mathematics who teaches eighth graders, and students themselves. The research design used is one shot case study. The result shows that: management learning has been very well implemented by the teacher; every students’ activity has been carried out by students; the students’ learning achievement have reached the criteria of completeness, and the students’ responses can be considered as positive. Therefore, it can be concluded that mathematic learning using OEQGF is effective.}
}

@article{wulf2010feedback,
  author    = {Wulf, Gabriele and Shea, Charles and Lewthwaite, Rebecca},
  title     = {Motor skill learning and performance: a review of influential factors},
  journal   = {Medical Education},
  year      = {2010},
  volume    = {44},
  number    = {1},
  pages     = {75--84},
  month     = {Jan},
  doi       = {10.1111/j.1365-2923.2009.03421.x},
  issn      = {1365-2923 (Electronic); 0308-0110 (Linking)},
  pmid      = {20078758},
  address   = {Department of Kinesiology and Nutrition Sciences, University of Nevada, Las Vegas, Nevada 89154-3034, USA},
  email     = {Gabriele.wulf@unlv.edu},
  keywords  = {Attention, Education, Medical/methods, Feedback, Psychological, Humans, Motor Skills, Observation, Practice, Psychological, Psychomotor Performance},
  language  = {eng},
  note      = {PMID: 20078758},
  publisher = {England},
  type      = {Journal Article; Review}
}

@article{zhou2023codebertscore,
  url       = {https://arxiv.org/abs/2302.05527},
  author    = {Zhou, Shuyan and Alon, Uri and Agarwal, Sumit and Neubig, Graham},
  title     = {CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code},
  publisher = {arXiv},
  year      = {2023}
}

@book{newman2015building,
  title     = {Building Microservices: Designing Fine-Grained Systems},
  author    = {Newman, Sam},
  edition   = {First},
  publisher = {O'Reilly Media},
  address   = {Beijing Sebastopol, CA},
  year      = {2015}
}

@article{hossain2023microservice,
  title    = {The role of microservice approach in edge computing: Opportunities, challenges, and research directions},
  journal  = {ICT Express},
  year     = {2023},
  issn     = {2405-9595},
  doi      = {https://doi.org/10.1016/j.icte.2023.06.006},
  url      = {https://www.sciencedirect.com/science/article/pii/S2405959523000760},
  author   = {Md. Delowar Hossain and Tangina Sultana and Sharmen Akhter and Md Imtiaz Hossain and Ngo Thien Thu and Luan N.T. Huynh and Ga-Won Lee and Eui-Nam Huh},
  keywords = {Edge computing, Microservices, Monolithic architectures, Microservice security, AI},
  abstract = {Edge computing has emerged as a promising computing paradigm that enables real-time data processing and analysis closer to the data source and boosts decision-making applications in a safe manner. On the other hand, the microservice is a new type of architecture that can be dynamically deployed, migrating across edge clouds on demand. Therefore, the combination of these two technologies can provide numerous benefits, including improved performance, reduced latency, and better resource utilization. In this paper, we present a thorough analysis of state-of-the-art research on the use of microservices in edge computing environments. We take into consideration several distinct microservice research directions, including coordination, orchestration, repositories, scheduling, autoscaling, deployment, resource management, and different security issues. Furthermore, we explore the potential applications of microservices in edge computing across various domains. Finally, the unsolved research issues and future directions of emerging trends in this area are also discussed.}
}

@inproceedings{aldebagy2018comparative,
  author    = {Al-Debagy, Omar and Martinek, Peter},
  booktitle = {2018 IEEE 18th International Symposium on Computational Intelligence and Informatics (CINTI)},
  title     = {A Comparative Review of Microservices and Monolithic Architectures},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {000149-000154},
  abstract  = {Microservices' architecture is getting attention in the academic community and the industry, and mostly is compared with monolithic architecture. Plenty of the results of these research papers contradict each other regarding the performance of these architectures. Therefore, these two architectures are compared in this paper, and some specific configurations of microservices' applications are evaluated as well in the term of service discovery. Monolithic architecture in concurrency testing showed better performance in throughput by 6% when compared to microservices architecture. The load testing scenario did not present significant difference between the two architectures. Furthermore, a third test comparing microservices applications built with different service discovery technologies such as Consul and Eureka showed that applications with Consul presented better results in terms of throughput.},
  keywords  = {},
  doi       = {10.1109/CINTI.2018.8928192},
  issn      = {2471-9269},
  month     = {Nov}
}
